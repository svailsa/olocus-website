"use strict";(globalThis.webpackChunkolocus_docs=globalThis.webpackChunkolocus_docs||[]).push([[2275],{7168:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>r,contentTitle:()=>l,default:()=>_,frontMatter:()=>i,metadata:()=>a,toc:()=>c});const a=JSON.parse('{"id":"extensions/infrastructure/storage","title":"Storage Backends","description":"The Storage extension provides multiple backend implementations for persisting Olocus Protocol blocks with different performance, durability, and deployment characteristics.","source":"@site/docs/extensions/infrastructure/storage.md","sourceDirName":"extensions/infrastructure","slug":"/extensions/infrastructure/storage","permalink":"/docs/extensions/infrastructure/storage","draft":false,"unlisted":false,"editUrl":"https://codeberg.org/olocus/protocol/edit/main/docs/extensions/infrastructure/storage.md","tags":[],"version":"current","lastUpdatedAt":1764947319000,"sidebarPosition":1,"frontMatter":{"id":"storage","title":"Storage Backends","sidebar_position":1},"sidebar":"docsSidebar","previous":{"title":"Zero Knowledge","permalink":"/docs/extensions/privacy/zero-knowledge"},"next":{"title":"Network","permalink":"/docs/extensions/infrastructure/network"}}');var s=t(4848),o=t(8453);const i={id:"storage",title:"Storage Backends",sidebar_position:1},l="Storage Backends",r={},c=[{value:"Overview",id:"overview",level:2},{value:"Storage Trait Interface",id:"storage-trait-interface",level:2},{value:"Core Storage Trait",id:"core-storage-trait",level:3},{value:"Storage Statistics",id:"storage-statistics",level:3},{value:"Memory Backend",id:"memory-backend",level:2},{value:"In-Memory Implementation",id:"in-memory-implementation",level:3},{value:"Filesystem Backend",id:"filesystem-backend",level:2},{value:"File-based Implementation",id:"file-based-implementation",level:3},{value:"RocksDB Backend",id:"rocksdb-backend",level:2},{value:"High-Performance Key-Value Store",id:"high-performance-key-value-store",level:3},{value:"SQLite Backend",id:"sqlite-backend",level:2},{value:"Relational Database Implementation",id:"relational-database-implementation",level:3},{value:"Caching Layer",id:"caching-layer",level:2},{value:"LRU Cache Implementation",id:"lru-cache-implementation",level:3},{value:"Integration Examples",id:"integration-examples",level:2},{value:"Multi-Backend Storage Manager",id:"multi-backend-storage-manager",level:3},{value:"Performance Benchmarks",id:"performance-benchmarks",level:2},{value:"Related Documentation",id:"related-documentation",level:2}];function d(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"storage-backends",children:"Storage Backends"})}),"\n",(0,s.jsx)(n.p,{children:"The Storage extension provides multiple backend implementations for persisting Olocus Protocol blocks with different performance, durability, and deployment characteristics."}),"\n",(0,s.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,s.jsx)(n.p,{children:"The storage extension supports various backend types optimized for different use cases:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Memory"}),": High-performance in-memory storage for testing and caching"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Filesystem"}),": Simple file-based storage for local development"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"RocksDB"}),": High-performance embedded key-value store"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"SQLite"}),": Lightweight relational database with SQL queries"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Distributed"}),": Integration with distributed storage systems"]}),"\n"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-rust",children:'use olocus_storage::*;\n\n// Configure storage backend\nlet storage_config = StorageConfig {\n    backend: StorageBackend::RocksDB {\n        path: "./olocus_data".to_string(),\n        compression: CompressionType::Zstd,\n        cache_size: 64 * 1024 * 1024, // 64MB cache\n    },\n    wal_config: WALConfig {\n        enabled: true,\n        sync_interval: Duration::from_secs(5),\n        max_log_size: 100 * 1024 * 1024, // 100MB\n    },\n    cache_config: CacheConfig {\n        block_cache_size: 32 * 1024 * 1024, // 32MB\n        index_cache_size: 8 * 1024 * 1024,  // 8MB\n        eviction_policy: EvictionPolicy::LRU,\n    },\n};\n\nlet storage = Storage::new(storage_config).await?;\n'})}),"\n",(0,s.jsx)(n.h2,{id:"storage-trait-interface",children:"Storage Trait Interface"}),"\n",(0,s.jsx)(n.h3,{id:"core-storage-trait",children:"Core Storage Trait"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-rust",children:"use olocus_storage::traits::*;\nuse olocus_core::*;\n\n#[async_trait::async_trait]\npub trait StorageBackend: Send + Sync {\n    async fn store_block(&mut self, block: &Block<impl BlockPayload>) -> Result<BlockHash>;\n    async fn retrieve_block(&self, hash: &BlockHash) -> Result<Option<Block<serde_json::Value>>>;\n    async fn delete_block(&mut self, hash: &BlockHash) -> Result<bool>;\n    async fn block_exists(&self, hash: &BlockHash) -> Result<bool>;\n    \n    // Batch operations\n    async fn store_blocks(&mut self, blocks: &[Block<impl BlockPayload>]) -> Result<Vec<BlockHash>>;\n    async fn retrieve_blocks(&self, hashes: &[BlockHash]) -> Result<Vec<Option<Block<serde_json::Value>>>>;\n    \n    // Iteration and querying\n    async fn list_blocks(&self, options: &ListOptions) -> Result<Vec<BlockMetadata>>;\n    async fn count_blocks(&self) -> Result<u64>;\n    \n    // Chain operations\n    async fn get_chain_head(&self) -> Result<Option<BlockHash>>;\n    async fn set_chain_head(&mut self, hash: &BlockHash) -> Result<()>;\n    async fn get_blocks_by_range(&self, start: u64, end: u64) -> Result<Vec<Block<serde_json::Value>>>;\n    \n    // Maintenance\n    async fn compact(&mut self) -> Result<()>;\n    async fn vacuum(&mut self) -> Result<()>;\n    async fn get_stats(&self) -> Result<StorageStats>;\n    async fn close(&mut self) -> Result<()>;\n}\n\n#[derive(Debug, Clone)]\npub struct ListOptions {\n    pub limit: Option<u64>,\n    pub offset: Option<u64>,\n    pub start_time: Option<SystemTime>,\n    pub end_time: Option<SystemTime>,\n    pub order: SortOrder,\n    pub filter: Option<BlockFilter>,\n}\n\n#[derive(Debug, Clone)]\npub enum SortOrder {\n    Ascending,\n    Descending,\n    ByTimestamp,\n    ByIndex,\n}\n\n#[derive(Debug, Clone)]\npub struct BlockFilter {\n    pub payload_type: Option<String>,\n    pub public_key: Option<PublicKey>,\n    pub min_index: Option<u64>,\n    pub max_index: Option<u64>,\n}\n\n#[derive(Debug, Clone)]\npub struct BlockMetadata {\n    pub hash: BlockHash,\n    pub index: u64,\n    pub timestamp: SystemTime,\n    pub payload_type: String,\n    pub public_key: PublicKey,\n    pub size_bytes: u64,\n}\n"})}),"\n",(0,s.jsx)(n.h3,{id:"storage-statistics",children:"Storage Statistics"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-rust",children:"#[derive(Debug, Clone)]\npub struct StorageStats {\n    pub total_blocks: u64,\n    pub total_size_bytes: u64,\n    pub oldest_block: Option<SystemTime>,\n    pub newest_block: Option<SystemTime>,\n    pub backend_specific: BackendStats,\n}\n\n#[derive(Debug, Clone)]\npub enum BackendStats {\n    Memory {\n        heap_usage: u64,\n    },\n    Filesystem {\n        directory_size: u64,\n        file_count: u64,\n    },\n    RocksDB {\n        live_data_size: u64,\n        total_data_size: u64,\n        num_entries: u64,\n        num_deletions: u64,\n        compaction_stats: CompactionStats,\n    },\n    SQLite {\n        database_size: u64,\n        page_count: u64,\n        page_size: u64,\n        fragmentation_ratio: f64,\n    },\n}\n\n#[derive(Debug, Clone)]\npub struct CompactionStats {\n    pub bytes_read: u64,\n    pub bytes_written: u64,\n    pub compaction_time: Duration,\n    pub last_compaction: SystemTime,\n}\n"})}),"\n",(0,s.jsx)(n.h2,{id:"memory-backend",children:"Memory Backend"}),"\n",(0,s.jsx)(n.h3,{id:"in-memory-implementation",children:"In-Memory Implementation"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-rust",children:"use olocus_storage::memory::*;\nuse std::collections::BTreeMap;\n\npub struct MemoryBackend {\n    blocks: BTreeMap<BlockHash, SerializedBlock>,\n    metadata: BTreeMap<BlockHash, BlockMetadata>,\n    index_to_hash: BTreeMap<u64, BlockHash>,\n    chain_head: Option<BlockHash>,\n    stats: StorageStats,\n}\n\n#[derive(Debug, Clone)]\npub struct SerializedBlock {\n    pub data: Vec<u8>,\n    pub format: WireFormat,\n    pub stored_at: SystemTime,\n}\n\nimpl MemoryBackend {\n    pub fn new() -> Self {\n        Self {\n            blocks: BTreeMap::new(),\n            metadata: BTreeMap::new(),\n            index_to_hash: BTreeMap::new(),\n            chain_head: None,\n            stats: StorageStats::default(),\n        }\n    }\n    \n    pub fn with_capacity(capacity: usize) -> Self {\n        // Pre-allocate memory for better performance\n        let mut backend = Self::new();\n        backend.blocks.reserve(capacity);\n        backend.metadata.reserve(capacity);\n        backend.index_to_hash.reserve(capacity);\n        backend\n    }\n}\n\n#[async_trait::async_trait]\nimpl StorageBackend for MemoryBackend {\n    async fn store_block(&mut self, block: &Block<impl BlockPayload>) -> Result<BlockHash> {\n        let hash = block.hash();\n        \n        // Serialize block using default wire format\n        let wire_format = WireFormat::default();\n        let serialized_data = wire_format.encode(block)?;\n        \n        let serialized_block = SerializedBlock {\n            data: serialized_data,\n            format: wire_format,\n            stored_at: SystemTime::now(),\n        };\n        \n        // Create metadata\n        let metadata = BlockMetadata {\n            hash: hash.clone(),\n            index: block.index,\n            timestamp: block.timestamp,\n            payload_type: std::any::type_name::<block::Payload>().to_string(),\n            public_key: block.public_key.clone(),\n            size_bytes: serialized_block.data.len() as u64,\n        };\n        \n        // Store block and metadata\n        self.blocks.insert(hash.clone(), serialized_block);\n        self.metadata.insert(hash.clone(), metadata);\n        self.index_to_hash.insert(block.index, hash.clone());\n        \n        // Update statistics\n        self.stats.total_blocks += 1;\n        self.stats.total_size_bytes += serialized_block.data.len() as u64;\n        \n        if self.stats.oldest_block.is_none() || \n           Some(block.timestamp) < self.stats.oldest_block {\n            self.stats.oldest_block = Some(block.timestamp);\n        }\n        \n        if self.stats.newest_block.is_none() || \n           Some(block.timestamp) > self.stats.newest_block {\n            self.stats.newest_block = Some(block.timestamp);\n        }\n        \n        Ok(hash)\n    }\n    \n    async fn retrieve_block(&self, hash: &BlockHash) -> Result<Option<Block<serde_json::Value>>> {\n        match self.blocks.get(hash) {\n            Some(serialized_block) => {\n                // Deserialize using the stored format\n                let block = serialized_block.format.decode::<Block<serde_json::Value>>(&serialized_block.data)?;\n                Ok(Some(block))\n            },\n            None => Ok(None),\n        }\n    }\n    \n    async fn delete_block(&mut self, hash: &BlockHash) -> Result<bool> {\n        let metadata = self.metadata.remove(hash);\n        let serialized = self.blocks.remove(hash);\n        \n        if let (Some(meta), Some(ser)) = (metadata, serialized) {\n            // Update index mapping\n            self.index_to_hash.remove(&meta.index);\n            \n            // Update statistics\n            self.stats.total_blocks -= 1;\n            self.stats.total_size_bytes -= ser.data.len() as u64;\n            \n            Ok(true)\n        } else {\n            Ok(false)\n        }\n    }\n    \n    async fn list_blocks(&self, options: &ListOptions) -> Result<Vec<BlockMetadata>> {\n        let mut results: Vec<BlockMetadata> = self.metadata.values().cloned().collect();\n        \n        // Apply filters\n        if let Some(ref filter) = options.filter {\n            results.retain(|meta| {\n                if let Some(ref payload_type) = filter.payload_type {\n                    if meta.payload_type != *payload_type {\n                        return false;\n                    }\n                }\n                \n                if let Some(ref public_key) = filter.public_key {\n                    if meta.public_key != *public_key {\n                        return false;\n                    }\n                }\n                \n                if let Some(min_index) = filter.min_index {\n                    if meta.index < min_index {\n                        return false;\n                    }\n                }\n                \n                if let Some(max_index) = filter.max_index {\n                    if meta.index > max_index {\n                        return false;\n                    }\n                }\n                \n                true\n            });\n        }\n        \n        // Apply time range filters\n        if let Some(start_time) = options.start_time {\n            results.retain(|meta| meta.timestamp >= start_time);\n        }\n        \n        if let Some(end_time) = options.end_time {\n            results.retain(|meta| meta.timestamp <= end_time);\n        }\n        \n        // Sort results\n        match options.order {\n            SortOrder::Ascending => results.sort_by_key(|m| m.index),\n            SortOrder::Descending => results.sort_by(|a, b| b.index.cmp(&a.index)),\n            SortOrder::ByTimestamp => results.sort_by_key(|m| m.timestamp),\n            SortOrder::ByIndex => results.sort_by_key(|m| m.index),\n        }\n        \n        // Apply pagination\n        if let Some(offset) = options.offset {\n            results = results.into_iter().skip(offset as usize).collect();\n        }\n        \n        if let Some(limit) = options.limit {\n            results.truncate(limit as usize);\n        }\n        \n        Ok(results)\n    }\n    \n    async fn get_stats(&self) -> Result<StorageStats> {\n        let heap_usage = self.blocks.values()\n            .map(|b| b.data.len() as u64)\n            .sum::<u64>()\n            + self.metadata.len() as u64 * 200; // Rough estimate for metadata overhead\n            \n        Ok(StorageStats {\n            total_blocks: self.stats.total_blocks,\n            total_size_bytes: self.stats.total_size_bytes,\n            oldest_block: self.stats.oldest_block,\n            newest_block: self.stats.newest_block,\n            backend_specific: BackendStats::Memory { heap_usage },\n        })\n    }\n}\n"})}),"\n",(0,s.jsx)(n.h2,{id:"filesystem-backend",children:"Filesystem Backend"}),"\n",(0,s.jsx)(n.h3,{id:"file-based-implementation",children:"File-based Implementation"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-rust",children:'use olocus_storage::filesystem::*;\nuse tokio::fs;\nuse std::path::PathBuf;\n\npub struct FilesystemBackend {\n    base_path: PathBuf,\n    index_file: IndexFile,\n    compression: CompressionType,\n}\n\n#[derive(Debug, Clone)]\npub enum CompressionType {\n    None,\n    Gzip,\n    Zstd,\n    Lz4,\n}\n\n#[derive(Debug)]\npub struct IndexFile {\n    path: PathBuf,\n    entries: BTreeMap<BlockHash, IndexEntry>,\n    dirty: bool,\n}\n\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct IndexEntry {\n    pub hash: BlockHash,\n    pub file_path: String,\n    pub file_offset: u64,\n    pub compressed_size: u64,\n    pub uncompressed_size: u64,\n    pub metadata: BlockMetadata,\n}\n\nimpl FilesystemBackend {\n    pub async fn new(base_path: impl Into<PathBuf>, compression: CompressionType) -> Result<Self> {\n        let base_path = base_path.into();\n        \n        // Create base directory if it doesn\'t exist\n        fs::create_dir_all(&base_path).await?;\n        \n        // Create subdirectories\n        let blocks_dir = base_path.join("blocks");\n        let index_dir = base_path.join("index");\n        fs::create_dir_all(&blocks_dir).await?;\n        fs::create_dir_all(&index_dir).await?;\n        \n        // Load or create index file\n        let index_file_path = index_dir.join("blocks.idx");\n        let index_file = IndexFile::load_or_create(index_file_path).await?;\n        \n        Ok(Self {\n            base_path,\n            index_file,\n            compression,\n        })\n    }\n    \n    fn get_block_file_path(&self, hash: &BlockHash) -> PathBuf {\n        let hash_str = hex::encode(hash);\n        // Use first 2 characters for directory sharding\n        let dir = &hash_str[0..2];\n        let filename = &hash_str[2..];\n        \n        self.base_path\n            .join("blocks")\n            .join(dir)\n            .join(format!("{}.blk", filename))\n    }\n    \n    async fn compress_data(&self, data: &[u8]) -> Result<Vec<u8>> {\n        match self.compression {\n            CompressionType::None => Ok(data.to_vec()),\n            CompressionType::Gzip => {\n                use flate2::Compression;\n                use flate2::write::GzEncoder;\n                use std::io::Write;\n                \n                let mut encoder = GzEncoder::new(Vec::new(), Compression::default());\n                encoder.write_all(data)?;\n                Ok(encoder.finish()?)\n            },\n            CompressionType::Zstd => {\n                use zstd::encode_all;\n                Ok(encode_all(data, 3)?) // Level 3 compression\n            },\n            CompressionType::Lz4 => {\n                use lz4_flex::compress_prepend_size;\n                Ok(compress_prepend_size(data))\n            }\n        }\n    }\n    \n    async fn decompress_data(&self, compressed: &[u8]) -> Result<Vec<u8>> {\n        match self.compression {\n            CompressionType::None => Ok(compressed.to_vec()),\n            CompressionType::Gzip => {\n                use flate2::read::GzDecoder;\n                use std::io::Read;\n                \n                let mut decoder = GzDecoder::new(compressed);\n                let mut data = Vec::new();\n                decoder.read_to_end(&mut data)?;\n                Ok(data)\n            },\n            CompressionType::Zstd => {\n                use zstd::decode_all;\n                Ok(decode_all(compressed)?)\n            },\n            CompressionType::Lz4 => {\n                use lz4_flex::decompress_size_prepended;\n                Ok(decompress_size_prepended(compressed)?)\n            }\n        }\n    }\n}\n\n#[async_trait::async_trait]\nimpl StorageBackend for FilesystemBackend {\n    async fn store_block(&mut self, block: &Block<impl BlockPayload>) -> Result<BlockHash> {\n        let hash = block.hash();\n        \n        // Check if block already exists\n        if self.index_file.entries.contains_key(&hash) {\n            return Ok(hash);\n        }\n        \n        // Serialize block\n        let wire_format = WireFormat::default();\n        let serialized_data = wire_format.encode(block)?;\n        \n        // Compress if enabled\n        let compressed_data = self.compress_data(&serialized_data).await?;\n        \n        // Determine file path\n        let file_path = self.get_block_file_path(&hash);\n        \n        // Create directory if needed\n        if let Some(parent) = file_path.parent() {\n            fs::create_dir_all(parent).await?;\n        }\n        \n        // Write to file\n        fs::write(&file_path, &compressed_data).await?;\n        \n        // Create index entry\n        let index_entry = IndexEntry {\n            hash: hash.clone(),\n            file_path: file_path.to_string_lossy().to_string(),\n            file_offset: 0,\n            compressed_size: compressed_data.len() as u64,\n            uncompressed_size: serialized_data.len() as u64,\n            metadata: BlockMetadata {\n                hash: hash.clone(),\n                index: block.index,\n                timestamp: block.timestamp,\n                payload_type: std::any::type_name::<block::Payload>().to_string(),\n                public_key: block.public_key.clone(),\n                size_bytes: serialized_data.len() as u64,\n            },\n        };\n        \n        // Add to index\n        self.index_file.entries.insert(hash.clone(), index_entry);\n        self.index_file.dirty = true;\n        \n        // Periodically flush index\n        if self.index_file.entries.len() % 100 == 0 {\n            self.index_file.flush().await?;\n        }\n        \n        Ok(hash)\n    }\n    \n    async fn retrieve_block(&self, hash: &BlockHash) -> Result<Option<Block<serde_json::Value>>> {\n        match self.index_file.entries.get(hash) {\n            Some(entry) => {\n                // Read compressed data\n                let compressed_data = fs::read(&entry.file_path).await?;\n                \n                // Decompress\n                let serialized_data = self.decompress_data(&compressed_data).await?;\n                \n                // Deserialize\n                let wire_format = WireFormat::default();\n                let block = wire_format.decode::<Block<serde_json::Value>>(&serialized_data)?;\n                \n                Ok(Some(block))\n            },\n            None => Ok(None),\n        }\n    }\n    \n    async fn delete_block(&mut self, hash: &BlockHash) -> Result<bool> {\n        match self.index_file.entries.remove(hash) {\n            Some(entry) => {\n                // Delete file\n                if let Err(e) = fs::remove_file(&entry.file_path).await {\n                    // Log error but don\'t fail - file might already be deleted\n                    eprintln!("Warning: Failed to delete block file {}: {}", entry.file_path, e);\n                }\n                \n                self.index_file.dirty = true;\n                Ok(true)\n            },\n            None => Ok(false),\n        }\n    }\n    \n    async fn get_stats(&self) -> Result<StorageStats> {\n        let total_blocks = self.index_file.entries.len() as u64;\n        \n        let (total_size, oldest, newest) = self.index_file.entries.values()\n            .fold((0u64, None, None), |(size, oldest, newest), entry| {\n                let new_size = size + entry.uncompressed_size;\n                let new_oldest = match oldest {\n                    None => Some(entry.metadata.timestamp),\n                    Some(old) => Some(old.min(entry.metadata.timestamp)),\n                };\n                let new_newest = match newest {\n                    None => Some(entry.metadata.timestamp),\n                    Some(new) => Some(new.max(entry.metadata.timestamp)),\n                };\n                (new_size, new_oldest, new_newest)\n            });\n        \n        // Calculate directory size\n        let directory_size = self.calculate_directory_size(&self.base_path).await?;\n        let file_count = self.count_files(&self.base_path).await?;\n        \n        Ok(StorageStats {\n            total_blocks,\n            total_size_bytes: total_size,\n            oldest_block: oldest,\n            newest_block: newest,\n            backend_specific: BackendStats::Filesystem {\n                directory_size,\n                file_count,\n            },\n        })\n    }\n    \n    async fn compact(&mut self) -> Result<()> {\n        // Flush index to ensure consistency\n        self.index_file.flush().await?;\n        \n        // Remove any orphaned files\n        self.cleanup_orphaned_files().await?;\n        \n        Ok(())\n    }\n}\n\nimpl IndexFile {\n    async fn load_or_create(path: PathBuf) -> Result<Self> {\n        let entries = if path.exists() {\n            let data = fs::read(&path).await?;\n            serde_json::from_slice(&data)?\n        } else {\n            BTreeMap::new()\n        };\n        \n        Ok(Self {\n            path,\n            entries,\n            dirty: false,\n        })\n    }\n    \n    async fn flush(&mut self) -> Result<()> {\n        if !self.dirty {\n            return Ok(());\n        }\n        \n        let data = serde_json::to_vec_pretty(&self.entries)?;\n        \n        // Atomic write using temporary file\n        let temp_path = self.path.with_extension("tmp");\n        fs::write(&temp_path, &data).await?;\n        fs::rename(&temp_path, &self.path).await?;\n        \n        self.dirty = false;\n        Ok(())\n    }\n}\n'})}),"\n",(0,s.jsx)(n.h2,{id:"rocksdb-backend",children:"RocksDB Backend"}),"\n",(0,s.jsx)(n.h3,{id:"high-performance-key-value-store",children:"High-Performance Key-Value Store"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-rust",children:'use olocus_storage::rocksdb::*;\nuse rocksdb::{DB, Options, WriteBatch, IteratorMode, Direction};\n\npub struct RocksDBBackend {\n    db: DB,\n    cf_blocks: rocksdb::ColumnFamily,\n    cf_metadata: rocksdb::ColumnFamily,\n    cf_index: rocksdb::ColumnFamily,\n    compression: CompressionType,\n}\n\n#[derive(Debug, Clone)]\npub struct RocksDBConfig {\n    pub path: String,\n    pub compression: CompressionType,\n    pub cache_size: usize,\n    pub write_buffer_size: usize,\n    pub max_write_buffer_number: i32,\n    pub max_background_jobs: i32,\n    pub enable_bloom_filter: bool,\n    pub bloom_filter_bits: f64,\n}\n\nimpl Default for RocksDBConfig {\n    fn default() -> Self {\n        Self {\n            path: "./olocus_rocksdb".to_string(),\n            compression: CompressionType::Zstd,\n            cache_size: 64 * 1024 * 1024,      // 64MB\n            write_buffer_size: 16 * 1024 * 1024, // 16MB\n            max_write_buffer_number: 3,\n            max_background_jobs: 4,\n            enable_bloom_filter: true,\n            bloom_filter_bits: 10.0,\n        }\n    }\n}\n\nimpl RocksDBBackend {\n    pub fn new(config: RocksDBConfig) -> Result<Self> {\n        let mut opts = Options::default();\n        opts.create_if_missing(true);\n        opts.create_missing_column_families(true);\n        \n        // Performance tuning\n        opts.set_write_buffer_size(config.write_buffer_size);\n        opts.set_max_write_buffer_number(config.max_write_buffer_number);\n        opts.set_max_background_jobs(config.max_background_jobs);\n        \n        // Compression\n        match config.compression {\n            CompressionType::None => opts.set_compression_type(rocksdb::DBCompressionType::None),\n            CompressionType::Zstd => opts.set_compression_type(rocksdb::DBCompressionType::Zstd),\n            CompressionType::Lz4 => opts.set_compression_type(rocksdb::DBCompressionType::Lz4),\n            _ => opts.set_compression_type(rocksdb::DBCompressionType::Snappy),\n        }\n        \n        // Block cache\n        if config.cache_size > 0 {\n            let cache = rocksdb::Cache::new_lru_cache(config.cache_size)?;\n            let mut block_opts = rocksdb::BlockBasedOptions::default();\n            block_opts.set_block_cache(&cache);\n            \n            if config.enable_bloom_filter {\n                block_opts.set_bloom_filter(config.bloom_filter_bits, false);\n            }\n            \n            opts.set_block_based_table_factory(&block_opts);\n        }\n        \n        // Column family descriptors\n        let cf_names = vec!["blocks", "metadata", "index"];\n        let cf_descriptors: Vec<rocksdb::ColumnFamilyDescriptor> = cf_names\n            .into_iter()\n            .map(|name| {\n                let mut cf_opts = Options::default();\n                cf_opts.set_compression_type(opts.get_compression_type());\n                rocksdb::ColumnFamilyDescriptor::new(name, cf_opts)\n            })\n            .collect();\n        \n        // Open database\n        let db = DB::open_cf_descriptors(&opts, &config.path, cf_descriptors)?;\n        \n        let cf_blocks = db.cf_handle("blocks")\n            .ok_or_else(|| StorageError::ColumnFamilyNotFound("blocks".to_string()))?;\n        let cf_metadata = db.cf_handle("metadata")\n            .ok_or_else(|| StorageError::ColumnFamilyNotFound("metadata".to_string()))?;\n        let cf_index = db.cf_handle("index")\n            .ok_or_else(|| StorageError::ColumnFamilyNotFound("index".to_string()))?;\n        \n        Ok(Self {\n            db,\n            cf_blocks,\n            cf_metadata,\n            cf_index,\n            compression: config.compression,\n        })\n    }\n    \n    fn block_key(&self, hash: &BlockHash) -> Vec<u8> {\n        format!("blk:{}", hex::encode(hash)).into_bytes()\n    }\n    \n    fn metadata_key(&self, hash: &BlockHash) -> Vec<u8> {\n        format!("meta:{}", hex::encode(hash)).into_bytes()\n    }\n    \n    fn index_key(&self, index: u64) -> Vec<u8> {\n        format!("idx:{:016x}", index).into_bytes()\n    }\n}\n\n#[async_trait::async_trait]\nimpl StorageBackend for RocksDBBackend {\n    async fn store_block(&mut self, block: &Block<impl BlockPayload>) -> Result<BlockHash> {\n        let hash = block.hash();\n        \n        // Check if block already exists\n        let block_key = self.block_key(&hash);\n        if self.db.key_may_exist_cf(self.cf_blocks, &block_key) {\n            if self.db.get_cf(self.cf_blocks, &block_key)?.is_some() {\n                return Ok(hash);\n            }\n        }\n        \n        // Serialize block\n        let wire_format = WireFormat::default();\n        let serialized_data = wire_format.encode(block)?;\n        \n        // Create metadata\n        let metadata = BlockMetadata {\n            hash: hash.clone(),\n            index: block.index,\n            timestamp: block.timestamp,\n            payload_type: std::any::type_name::<block::Payload>().to_string(),\n            public_key: block.public_key.clone(),\n            size_bytes: serialized_data.len() as u64,\n        };\n        let metadata_data = serde_json::to_vec(&metadata)?;\n        \n        // Prepare batch write for atomicity\n        let mut batch = WriteBatch::default();\n        \n        // Store block data\n        batch.put_cf(self.cf_blocks, &block_key, &serialized_data);\n        \n        // Store metadata\n        let metadata_key = self.metadata_key(&hash);\n        batch.put_cf(self.cf_metadata, &metadata_key, &metadata_data);\n        \n        // Store index mapping\n        let index_key = self.index_key(block.index);\n        batch.put_cf(self.cf_index, &index_key, &hash);\n        \n        // Atomic write\n        self.db.write(batch)?;\n        \n        Ok(hash)\n    }\n    \n    async fn retrieve_block(&self, hash: &BlockHash) -> Result<Option<Block<serde_json::Value>>> {\n        let block_key = self.block_key(hash);\n        \n        match self.db.get_cf(self.cf_blocks, &block_key)? {\n            Some(serialized_data) => {\n                let wire_format = WireFormat::default();\n                let block = wire_format.decode::<Block<serde_json::Value>>(&serialized_data)?;\n                Ok(Some(block))\n            },\n            None => Ok(None),\n        }\n    }\n    \n    async fn store_blocks(&mut self, blocks: &[Block<impl BlockPayload>]) -> Result<Vec<BlockHash>> {\n        let mut batch = WriteBatch::default();\n        let mut hashes = Vec::new();\n        \n        for block in blocks {\n            let hash = block.hash();\n            \n            // Serialize block\n            let wire_format = WireFormat::default();\n            let serialized_data = wire_format.encode(block)?;\n            \n            // Create metadata\n            let metadata = BlockMetadata {\n                hash: hash.clone(),\n                index: block.index,\n                timestamp: block.timestamp,\n                payload_type: std::any::type_name::<block::Payload>().to_string(),\n                public_key: block.public_key.clone(),\n                size_bytes: serialized_data.len() as u64,\n            };\n            let metadata_data = serde_json::to_vec(&metadata)?;\n            \n            // Add to batch\n            let block_key = self.block_key(&hash);\n            batch.put_cf(self.cf_blocks, &block_key, &serialized_data);\n            \n            let metadata_key = self.metadata_key(&hash);\n            batch.put_cf(self.cf_metadata, &metadata_key, &metadata_data);\n            \n            let index_key = self.index_key(block.index);\n            batch.put_cf(self.cf_index, &index_key, &hash);\n            \n            hashes.push(hash);\n        }\n        \n        // Atomic batch write\n        self.db.write(batch)?;\n        \n        Ok(hashes)\n    }\n    \n    async fn list_blocks(&self, options: &ListOptions) -> Result<Vec<BlockMetadata>> {\n        let mut results = Vec::new();\n        \n        // Iterate through metadata column family\n        let iter = self.db.iterator_cf(\n            self.cf_metadata,\n            IteratorMode::Start\n        );\n        \n        for item in iter {\n            let (_key, value) = item?;\n            let metadata: BlockMetadata = serde_json::from_slice(&value)?;\n            \n            // Apply filters\n            let mut include = true;\n            \n            if let Some(ref filter) = options.filter {\n                if let Some(ref payload_type) = filter.payload_type {\n                    if metadata.payload_type != *payload_type {\n                        include = false;\n                    }\n                }\n                \n                if let Some(ref public_key) = filter.public_key {\n                    if metadata.public_key != *public_key {\n                        include = false;\n                    }\n                }\n                \n                if let Some(min_index) = filter.min_index {\n                    if metadata.index < min_index {\n                        include = false;\n                    }\n                }\n                \n                if let Some(max_index) = filter.max_index {\n                    if metadata.index > max_index {\n                        include = false;\n                    }\n                }\n            }\n            \n            // Apply time range\n            if let Some(start_time) = options.start_time {\n                if metadata.timestamp < start_time {\n                    include = false;\n                }\n            }\n            \n            if let Some(end_time) = options.end_time {\n                if metadata.timestamp > end_time {\n                    include = false;\n                }\n            }\n            \n            if include {\n                results.push(metadata);\n            }\n        }\n        \n        // Sort results\n        match options.order {\n            SortOrder::Ascending => results.sort_by_key(|m| m.index),\n            SortOrder::Descending => results.sort_by(|a, b| b.index.cmp(&a.index)),\n            SortOrder::ByTimestamp => results.sort_by_key(|m| m.timestamp),\n            SortOrder::ByIndex => results.sort_by_key(|m| m.index),\n        }\n        \n        // Apply pagination\n        if let Some(offset) = options.offset {\n            results = results.into_iter().skip(offset as usize).collect();\n        }\n        \n        if let Some(limit) = options.limit {\n            results.truncate(limit as usize);\n        }\n        \n        Ok(results)\n    }\n    \n    async fn get_stats(&self) -> Result<StorageStats> {\n        // Get RocksDB statistics\n        let live_data_size = self.db.property_int_value("rocksdb.estimate-live-data-size")?\n            .unwrap_or(0);\n        let total_data_size = self.db.property_int_value("rocksdb.total-sst-files-size")?\n            .unwrap_or(0);\n        let num_entries = self.db.property_int_value("rocksdb.estimate-num-keys")?\n            .unwrap_or(0);\n        \n        // Count blocks by iterating metadata\n        let mut total_blocks = 0;\n        let mut total_size_bytes = 0;\n        let mut oldest_block = None;\n        let mut newest_block = None;\n        \n        let iter = self.db.iterator_cf(self.cf_metadata, IteratorMode::Start);\n        for item in iter {\n            let (_key, value) = item?;\n            let metadata: BlockMetadata = serde_json::from_slice(&value)?;\n            \n            total_blocks += 1;\n            total_size_bytes += metadata.size_bytes;\n            \n            if oldest_block.is_none() || Some(metadata.timestamp) < oldest_block {\n                oldest_block = Some(metadata.timestamp);\n            }\n            \n            if newest_block.is_none() || Some(metadata.timestamp) > newest_block {\n                newest_block = Some(metadata.timestamp);\n            }\n        }\n        \n        Ok(StorageStats {\n            total_blocks,\n            total_size_bytes,\n            oldest_block,\n            newest_block,\n            backend_specific: BackendStats::RocksDB {\n                live_data_size,\n                total_data_size,\n                num_entries,\n                num_deletions: 0, // Would need to track this separately\n                compaction_stats: CompactionStats {\n                    bytes_read: 0,\n                    bytes_written: 0,\n                    compaction_time: Duration::ZERO,\n                    last_compaction: SystemTime::now(),\n                },\n            },\n        })\n    }\n    \n    async fn compact(&mut self) -> Result<()> {\n        // Trigger manual compaction\n        self.db.compact_range::<&[u8], &[u8]>(None, None);\n        Ok(())\n    }\n    \n    async fn close(&mut self) -> Result<()> {\n        // RocksDB automatically closes when dropped\n        Ok(())\n    }\n}\n'})}),"\n",(0,s.jsx)(n.h2,{id:"sqlite-backend",children:"SQLite Backend"}),"\n",(0,s.jsx)(n.h3,{id:"relational-database-implementation",children:"Relational Database Implementation"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-rust",children:'use olocus_storage::sqlite::*;\nuse sqlx::{SqlitePool, Row};\n\npub struct SQLiteBackend {\n    pool: SqlitePool,\n    config: SQLiteConfig,\n}\n\n#[derive(Debug, Clone)]\npub struct SQLiteConfig {\n    pub database_path: String,\n    pub max_connections: u32,\n    pub journal_mode: JournalMode,\n    pub synchronous: SynchronousMode,\n    pub cache_size: i64,\n    pub temp_store: TempStore,\n    pub enable_wal: bool,\n}\n\n#[derive(Debug, Clone)]\npub enum JournalMode {\n    Delete,\n    Truncate,\n    Persist,\n    Memory,\n    WAL,\n}\n\n#[derive(Debug, Clone)]\npub enum SynchronousMode {\n    Off,\n    Normal,\n    Full,\n    Extra,\n}\n\n#[derive(Debug, Clone)]\npub enum TempStore {\n    Default,\n    File,\n    Memory,\n}\n\nimpl SQLiteBackend {\n    pub async fn new(config: SQLiteConfig) -> Result<Self> {\n        // Build connection string with pragmas\n        let mut connection_string = format!("sqlite:{}", config.database_path);\n        \n        // Configure SQLite pragmas\n        let pragmas = format!(\n            "?{}{}{}{}{}",\n            format!("journal_mode={}", config.journal_mode.to_string()),\n            format!("&synchronous={}", config.synchronous.to_string()),\n            format!("&cache_size={}", config.cache_size),\n            format!("&temp_store={}", config.temp_store.to_string()),\n            if config.enable_wal { "&wal_autocheckpoint=1000" } else { "" }\n        );\n        \n        connection_string.push_str(&pragmas);\n        \n        // Create connection pool\n        let pool = SqlitePool::connect(&connection_string).await?;\n        \n        // Create tables\n        let mut backend = Self { pool, config };\n        backend.create_tables().await?;\n        backend.create_indices().await?;\n        \n        Ok(backend)\n    }\n    \n    async fn create_tables(&self) -> Result<()> {\n        // Blocks table\n        sqlx::query(r#"\n            CREATE TABLE IF NOT EXISTS blocks (\n                hash TEXT PRIMARY KEY,\n                block_index INTEGER NOT NULL,\n                timestamp INTEGER NOT NULL,\n                public_key BLOB NOT NULL,\n                previous_hash TEXT,\n                payload_type TEXT NOT NULL,\n                payload_data BLOB NOT NULL,\n                signature BLOB NOT NULL,\n                size_bytes INTEGER NOT NULL,\n                created_at INTEGER NOT NULL DEFAULT (unixepoch())\n            )\n        "#)\n        .execute(&self.pool)\n        .await?;\n        \n        // Chain metadata table\n        sqlx::query(r#"\n            CREATE TABLE IF NOT EXISTS chain_metadata (\n                key TEXT PRIMARY KEY,\n                value TEXT NOT NULL\n            )\n        "#)\n        .execute(&self.pool)\n        .await?;\n        \n        // Block statistics table for analytics\n        sqlx::query(r#"\n            CREATE TABLE IF NOT EXISTS block_stats (\n                date TEXT PRIMARY KEY,\n                block_count INTEGER NOT NULL DEFAULT 0,\n                total_size INTEGER NOT NULL DEFAULT 0,\n                unique_keys INTEGER NOT NULL DEFAULT 0\n            )\n        "#)\n        .execute(&self.pool)\n        .await?;\n        \n        Ok(())\n    }\n    \n    async fn create_indices(&self) -> Result<()> {\n        // Index on block_index for chain operations\n        sqlx::query("CREATE INDEX IF NOT EXISTS idx_blocks_index ON blocks (block_index)")\n            .execute(&self.pool)\n            .await?;\n        \n        // Index on timestamp for time-based queries\n        sqlx::query("CREATE INDEX IF NOT EXISTS idx_blocks_timestamp ON blocks (timestamp)")\n            .execute(&self.pool)\n            .await?;\n        \n        // Index on public_key for filtering\n        sqlx::query("CREATE INDEX IF NOT EXISTS idx_blocks_public_key ON blocks (public_key)")\n            .execute(&self.pool)\n            .await?;\n        \n        // Index on payload_type for type-based queries\n        sqlx::query("CREATE INDEX IF NOT EXISTS idx_blocks_payload_type ON blocks (payload_type)")\n            .execute(&self.pool)\n            .await?;\n        \n        // Composite index for common query patterns\n        sqlx::query("CREATE INDEX IF NOT EXISTS idx_blocks_composite ON blocks (timestamp, payload_type, public_key)")\n            .execute(&self.pool)\n            .await?;\n        \n        Ok(())\n    }\n}\n\n#[async_trait::async_trait]\nimpl StorageBackend for SQLiteBackend {\n    async fn store_block(&mut self, block: &Block<impl BlockPayload>) -> Result<BlockHash> {\n        let hash = block.hash();\n        \n        // Check if block already exists\n        let exists = sqlx::query_scalar::<_, bool>("SELECT EXISTS(SELECT 1 FROM blocks WHERE hash = ?)")\n            .bind(hex::encode(&hash))\n            .fetch_one(&self.pool)\n            .await?;\n        \n        if exists {\n            return Ok(hash);\n        }\n        \n        // Serialize payload\n        let payload_data = serde_json::to_vec(&block.payload)?;\n        let signature_data = block.signature.as_bytes();\n        let public_key_data = block.public_key.as_bytes();\n        let previous_hash_hex = block.previous_hash.as_ref().map(|h| hex::encode(h));\n        \n        // Insert block\n        sqlx::query(r#"\n            INSERT INTO blocks (\n                hash, block_index, timestamp, public_key, previous_hash,\n                payload_type, payload_data, signature, size_bytes\n            ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)\n        "#)\n        .bind(hex::encode(&hash))\n        .bind(block.index as i64)\n        .bind(block.timestamp.duration_since(SystemTime::UNIX_EPOCH)?.as_secs() as i64)\n        .bind(public_key_data)\n        .bind(previous_hash_hex)\n        .bind(std::any::type_name::<block::Payload>())\n        .bind(payload_data)\n        .bind(signature_data)\n        .bind(block.size_bytes() as i64)\n        .execute(&self.pool)\n        .await?;\n        \n        // Update daily statistics\n        self.update_daily_stats().await?;\n        \n        Ok(hash)\n    }\n    \n    async fn retrieve_block(&self, hash: &BlockHash) -> Result<Option<Block<serde_json::Value>>> {\n        let row = sqlx::query(r#"\n            SELECT block_index, timestamp, public_key, previous_hash,\n                   payload_data, signature\n            FROM blocks \n            WHERE hash = ?\n        "#)\n        .bind(hex::encode(hash))\n        .fetch_optional(&self.pool)\n        .await?;\n        \n        match row {\n            Some(row) => {\n                let index: i64 = row.get("block_index");\n                let timestamp_secs: i64 = row.get("timestamp");\n                let public_key_bytes: Vec<u8> = row.get("public_key");\n                let previous_hash_hex: Option<String> = row.get("previous_hash");\n                let payload_data: Vec<u8> = row.get("payload_data");\n                let signature_bytes: Vec<u8> = row.get("signature");\n                \n                let timestamp = SystemTime::UNIX_EPOCH + Duration::from_secs(timestamp_secs as u64);\n                let public_key = PublicKey::from_bytes(&public_key_bytes)?;\n                let previous_hash = previous_hash_hex.map(|h| hex::decode(h)).transpose()?\n                    .map(|bytes| BlockHash::try_from(bytes.as_slice())).transpose()?;\n                let payload: serde_json::Value = serde_json::from_slice(&payload_data)?;\n                let signature = Signature::from_bytes(&signature_bytes)?;\n                \n                let block = Block {\n                    index: index as u64,\n                    timestamp,\n                    public_key,\n                    previous_hash,\n                    payload,\n                    signature,\n                };\n                \n                Ok(Some(block))\n            },\n            None => Ok(None),\n        }\n    }\n    \n    async fn list_blocks(&self, options: &ListOptions) -> Result<Vec<BlockMetadata>> {\n        let mut query = "SELECT hash, block_index, timestamp, payload_type, public_key, size_bytes FROM blocks".to_string();\n        let mut conditions = Vec::new();\n        let mut params = Vec::new();\n        \n        // Build WHERE clause\n        if let Some(ref filter) = options.filter {\n            if let Some(ref payload_type) = filter.payload_type {\n                conditions.push("payload_type = ?");\n                params.push(payload_type.as_str());\n            }\n            \n            if let Some(min_index) = filter.min_index {\n                conditions.push("block_index >= ?");\n                params.push(&min_index.to_string());\n            }\n            \n            if let Some(max_index) = filter.max_index {\n                conditions.push("block_index <= ?");\n                params.push(&max_index.to_string());\n            }\n        }\n        \n        if let Some(start_time) = options.start_time {\n            conditions.push("timestamp >= ?");\n            let timestamp = start_time.duration_since(SystemTime::UNIX_EPOCH)?.as_secs();\n            params.push(&timestamp.to_string());\n        }\n        \n        if let Some(end_time) = options.end_time {\n            conditions.push("timestamp <= ?");\n            let timestamp = end_time.duration_since(SystemTime::UNIX_EPOCH)?.as_secs();\n            params.push(&timestamp.to_string());\n        }\n        \n        if !conditions.is_empty() {\n            query.push_str(" WHERE ");\n            query.push_str(&conditions.join(" AND "));\n        }\n        \n        // Add ORDER BY\n        match options.order {\n            SortOrder::Ascending => query.push_str(" ORDER BY block_index ASC"),\n            SortOrder::Descending => query.push_str(" ORDER BY block_index DESC"),\n            SortOrder::ByTimestamp => query.push_str(" ORDER BY timestamp ASC"),\n            SortOrder::ByIndex => query.push_str(" ORDER BY block_index ASC"),\n        }\n        \n        // Add LIMIT and OFFSET\n        if let Some(limit) = options.limit {\n            query.push_str(&format!(" LIMIT {}", limit));\n        }\n        \n        if let Some(offset) = options.offset {\n            query.push_str(&format!(" OFFSET {}", offset));\n        }\n        \n        // Execute query\n        let mut query_builder = sqlx::query(&query);\n        for param in params {\n            query_builder = query_builder.bind(param);\n        }\n        \n        let rows = query_builder.fetch_all(&self.pool).await?;\n        \n        let mut results = Vec::new();\n        for row in rows {\n            let hash_hex: String = row.get("hash");\n            let hash = BlockHash::try_from(hex::decode(hash_hex)?.as_slice())?;\n            \n            let index: i64 = row.get("block_index");\n            let timestamp_secs: i64 = row.get("timestamp");\n            let payload_type: String = row.get("payload_type");\n            let public_key_bytes: Vec<u8> = row.get("public_key");\n            let size_bytes: i64 = row.get("size_bytes");\n            \n            let metadata = BlockMetadata {\n                hash,\n                index: index as u64,\n                timestamp: SystemTime::UNIX_EPOCH + Duration::from_secs(timestamp_secs as u64),\n                payload_type,\n                public_key: PublicKey::from_bytes(&public_key_bytes)?,\n                size_bytes: size_bytes as u64,\n            };\n            \n            results.push(metadata);\n        }\n        \n        Ok(results)\n    }\n    \n    async fn get_stats(&self) -> Result<StorageStats> {\n        // Get basic counts and sizes\n        let stats_row = sqlx::query(r#"\n            SELECT \n                COUNT(*) as total_blocks,\n                SUM(size_bytes) as total_size,\n                MIN(timestamp) as oldest_timestamp,\n                MAX(timestamp) as newest_timestamp\n            FROM blocks\n        "#)\n        .fetch_one(&self.pool)\n        .await?;\n        \n        let total_blocks: i64 = stats_row.get("total_blocks");\n        let total_size: Option<i64> = stats_row.get("total_size");\n        let oldest_timestamp: Option<i64> = stats_row.get("oldest_timestamp");\n        let newest_timestamp: Option<i64> = stats_row.get("newest_timestamp");\n        \n        let oldest_block = oldest_timestamp.map(|ts| {\n            SystemTime::UNIX_EPOCH + Duration::from_secs(ts as u64)\n        });\n        \n        let newest_block = newest_timestamp.map(|ts| {\n            SystemTime::UNIX_EPOCH + Duration::from_secs(ts as u64)\n        });\n        \n        // Get database file size and page info\n        let db_size_row = sqlx::query("PRAGMA page_count")\n            .fetch_one(&self.pool)\n            .await?;\n        let page_count: i64 = db_size_row.get("page_count");\n        \n        let page_size_row = sqlx::query("PRAGMA page_size")\n            .fetch_one(&self.pool)\n            .await?;\n        let page_size: i64 = page_size_row.get("page_size");\n        \n        let database_size = (page_count * page_size) as u64;\n        \n        // Calculate fragmentation (simplified)\n        let freelist_row = sqlx::query("PRAGMA freelist_count")\n            .fetch_one(&self.pool)\n            .await?;\n        let freelist_count: i64 = freelist_row.get("freelist_count");\n        \n        let fragmentation_ratio = if page_count > 0 {\n            freelist_count as f64 / page_count as f64\n        } else {\n            0.0\n        };\n        \n        Ok(StorageStats {\n            total_blocks: total_blocks as u64,\n            total_size_bytes: total_size.unwrap_or(0) as u64,\n            oldest_block,\n            newest_block,\n            backend_specific: BackendStats::SQLite {\n                database_size,\n                page_count: page_count as u64,\n                page_size: page_size as u64,\n                fragmentation_ratio,\n            },\n        })\n    }\n    \n    async fn vacuum(&mut self) -> Result<()> {\n        sqlx::query("VACUUM").execute(&self.pool).await?;\n        Ok(())\n    }\n    \n    async fn close(&mut self) -> Result<()> {\n        self.pool.close().await;\n        Ok(())\n    }\n}\n\nimpl SQLiteBackend {\n    async fn update_daily_stats(&self) -> Result<()> {\n        let today = chrono::Utc::now().date().format("%Y-%m-%d").to_string();\n        \n        sqlx::query(r#"\n            INSERT OR REPLACE INTO block_stats (date, block_count, total_size, unique_keys)\n            SELECT \n                ? as date,\n                COUNT(*) as block_count,\n                SUM(size_bytes) as total_size,\n                COUNT(DISTINCT public_key) as unique_keys\n            FROM blocks \n            WHERE date(timestamp, \'unixepoch\') = ?\n        "#)\n        .bind(&today)\n        .bind(&today)\n        .execute(&self.pool)\n        .await?;\n        \n        Ok(())\n    }\n    \n    pub async fn get_daily_stats(&self, date: &str) -> Result<Option<DailyStats>> {\n        let row = sqlx::query(r#"\n            SELECT block_count, total_size, unique_keys \n            FROM block_stats \n            WHERE date = ?\n        "#)\n        .bind(date)\n        .fetch_optional(&self.pool)\n        .await?;\n        \n        match row {\n            Some(row) => {\n                Ok(Some(DailyStats {\n                    date: date.to_string(),\n                    block_count: row.get::<i64, _>("block_count") as u64,\n                    total_size: row.get::<i64, _>("total_size") as u64,\n                    unique_keys: row.get::<i64, _>("unique_keys") as u64,\n                }))\n            },\n            None => Ok(None),\n        }\n    }\n}\n\n#[derive(Debug, Clone)]\npub struct DailyStats {\n    pub date: String,\n    pub block_count: u64,\n    pub total_size: u64,\n    pub unique_keys: u64,\n}\n'})}),"\n",(0,s.jsx)(n.h2,{id:"caching-layer",children:"Caching Layer"}),"\n",(0,s.jsx)(n.h3,{id:"lru-cache-implementation",children:"LRU Cache Implementation"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-rust",children:"use olocus_storage::cache::*;\nuse std::collections::HashMap;\nuse lru::LruCache;\n\npub struct CachedStorageBackend<B: StorageBackend> {\n    backend: B,\n    block_cache: LruCache<BlockHash, Arc<Block<serde_json::Value>>>,\n    metadata_cache: LruCache<BlockHash, BlockMetadata>,\n    config: CacheConfig,\n    stats: CacheStats,\n}\n\n#[derive(Debug, Clone)]\npub struct CacheConfig {\n    pub block_cache_size: usize,\n    pub metadata_cache_size: usize,\n    pub ttl: Duration,\n    pub eviction_policy: EvictionPolicy,\n    pub write_through: bool,\n    pub prefetch_enabled: bool,\n    pub prefetch_size: usize,\n}\n\n#[derive(Debug, Clone)]\npub enum EvictionPolicy {\n    LRU,\n    LFU,\n    FIFO,\n    Random,\n}\n\n#[derive(Debug, Clone, Default)]\npub struct CacheStats {\n    pub hits: u64,\n    pub misses: u64,\n    pub evictions: u64,\n    pub writes: u64,\n    pub hit_ratio: f64,\n}\n\nimpl<B: StorageBackend> CachedStorageBackend<B> {\n    pub fn new(backend: B, config: CacheConfig) -> Self {\n        Self {\n            backend,\n            block_cache: LruCache::new(config.block_cache_size),\n            metadata_cache: LruCache::new(config.metadata_cache_size),\n            config,\n            stats: CacheStats::default(),\n        }\n    }\n    \n    fn update_hit_ratio(&mut self) {\n        let total = self.stats.hits + self.stats.misses;\n        self.stats.hit_ratio = if total > 0 {\n            self.stats.hits as f64 / total as f64\n        } else {\n            0.0\n        };\n    }\n    \n    async fn prefetch_related_blocks(&mut self, hash: &BlockHash) -> Result<()> {\n        if !self.config.prefetch_enabled {\n            return Ok(());\n        }\n        \n        // Get metadata to find related blocks\n        if let Some(metadata) = self.metadata_cache.get(hash) {\n            // Prefetch blocks around this index\n            let start_index = metadata.index.saturating_sub(self.config.prefetch_size as u64 / 2);\n            let end_index = metadata.index + (self.config.prefetch_size as u64 / 2);\n            \n            let related_blocks = self.backend.get_blocks_by_range(start_index, end_index).await?;\n            \n            for block in related_blocks {\n                let block_hash = block.hash();\n                if !self.block_cache.contains(&block_hash) {\n                    self.block_cache.put(block_hash, Arc::new(block));\n                }\n            }\n        }\n        \n        Ok(())\n    }\n}\n\n#[async_trait::async_trait]\nimpl<B: StorageBackend> StorageBackend for CachedStorageBackend<B> {\n    async fn store_block(&mut self, block: &Block<impl BlockPayload>) -> Result<BlockHash> {\n        let hash = self.backend.store_block(block).await?;\n        \n        // Cache the block if write-through is enabled\n        if self.config.write_through {\n            // Convert to JSON payload for caching\n            let json_block = Block {\n                index: block.index,\n                timestamp: block.timestamp,\n                public_key: block.public_key.clone(),\n                previous_hash: block.previous_hash.clone(),\n                payload: serde_json::to_value(&block.payload)?,\n                signature: block.signature.clone(),\n            };\n            \n            self.block_cache.put(hash.clone(), Arc::new(json_block));\n            \n            // Cache metadata\n            let metadata = BlockMetadata {\n                hash: hash.clone(),\n                index: block.index,\n                timestamp: block.timestamp,\n                payload_type: std::any::type_name::<block::Payload>().to_string(),\n                public_key: block.public_key.clone(),\n                size_bytes: block.size_bytes(),\n            };\n            \n            self.metadata_cache.put(hash.clone(), metadata);\n        }\n        \n        self.stats.writes += 1;\n        Ok(hash)\n    }\n    \n    async fn retrieve_block(&self, hash: &BlockHash) -> Result<Option<Block<serde_json::Value>>> {\n        // Check cache first\n        if let Some(cached_block) = self.block_cache.get(hash) {\n            self.stats.hits += 1;\n            self.update_hit_ratio();\n            return Ok(Some((**cached_block).clone()));\n        }\n        \n        // Cache miss - fetch from backend\n        self.stats.misses += 1;\n        \n        match self.backend.retrieve_block(hash).await? {\n            Some(block) => {\n                // Cache the result\n                self.block_cache.put(hash.clone(), Arc::new(block.clone()));\n                \n                // Trigger prefetch if enabled\n                if self.config.prefetch_enabled {\n                    let _ = self.prefetch_related_blocks(hash).await;\n                }\n                \n                self.update_hit_ratio();\n                Ok(Some(block))\n            },\n            None => {\n                self.update_hit_ratio();\n                Ok(None)\n            }\n        }\n    }\n    \n    async fn list_blocks(&self, options: &ListOptions) -> Result<Vec<BlockMetadata>> {\n        // For list operations, we generally bypass cache and go to backend\n        // unless we have a comprehensive metadata cache strategy\n        self.backend.list_blocks(options).await\n    }\n    \n    async fn get_stats(&self) -> Result<StorageStats> {\n        let mut backend_stats = self.backend.get_stats().await?;\n        \n        // Add cache stats to backend stats\n        match &mut backend_stats.backend_specific {\n            BackendStats::Memory { ref mut heap_usage } => {\n                *heap_usage += self.estimate_cache_memory_usage();\n            },\n            _ => {\n                // For other backends, we could add cache-specific stats\n            }\n        }\n        \n        Ok(backend_stats)\n    }\n    \n    async fn compact(&mut self) -> Result<()> {\n        // Clear caches during compaction to ensure consistency\n        self.block_cache.clear();\n        self.metadata_cache.clear();\n        \n        self.backend.compact().await\n    }\n    \n    async fn close(&mut self) -> Result<()> {\n        self.block_cache.clear();\n        self.metadata_cache.clear();\n        self.backend.close().await\n    }\n}\n\nimpl<B: StorageBackend> CachedStorageBackend<B> {\n    pub fn get_cache_stats(&self) -> CacheStats {\n        self.stats.clone()\n    }\n    \n    pub fn clear_cache(&mut self) {\n        self.block_cache.clear();\n        self.metadata_cache.clear();\n        self.stats.evictions += self.stats.hits + self.stats.misses;\n        self.stats.hits = 0;\n        self.stats.misses = 0;\n        self.update_hit_ratio();\n    }\n    \n    fn estimate_cache_memory_usage(&self) -> u64 {\n        // Rough estimation of cache memory usage\n        let block_cache_size = self.block_cache.len() * 1024; // Estimate 1KB per cached block\n        let metadata_cache_size = self.metadata_cache.len() * 200; // Estimate 200B per metadata\n        \n        (block_cache_size + metadata_cache_size) as u64\n    }\n    \n    pub fn configure_cache(&mut self, new_config: CacheConfig) {\n        // Resize caches if needed\n        if new_config.block_cache_size != self.config.block_cache_size {\n            self.block_cache.resize(new_config.block_cache_size);\n        }\n        \n        if new_config.metadata_cache_size != self.config.metadata_cache_size {\n            self.metadata_cache.resize(new_config.metadata_cache_size);\n        }\n        \n        self.config = new_config;\n    }\n}\n"})}),"\n",(0,s.jsx)(n.h2,{id:"integration-examples",children:"Integration Examples"}),"\n",(0,s.jsx)(n.h3,{id:"multi-backend-storage-manager",children:"Multi-Backend Storage Manager"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-rust",children:'use olocus_storage::manager::*;\n\npub struct StorageManager {\n    primary: Box<dyn StorageBackend>,\n    replicas: Vec<Box<dyn StorageBackend>>,\n    config: StorageManagerConfig,\n}\n\n#[derive(Debug, Clone)]\npub struct StorageManagerConfig {\n    pub replication_factor: u32,\n    pub consistency_level: ConsistencyLevel,\n    pub health_check_interval: Duration,\n    pub auto_failover: bool,\n    pub sync_writes: bool,\n}\n\n#[derive(Debug, Clone)]\npub enum ConsistencyLevel {\n    One,        // Write to one backend\n    Quorum,     // Write to majority\n    All,        // Write to all backends\n}\n\nimpl StorageManager {\n    pub fn new(\n        primary: Box<dyn StorageBackend>,\n        replicas: Vec<Box<dyn StorageBackend>>,\n        config: StorageManagerConfig\n    ) -> Self {\n        Self {\n            primary,\n            replicas,\n            config,\n        }\n    }\n    \n    pub async fn store_with_replication(&mut self, block: &Block<impl BlockPayload>) -> Result<BlockHash> {\n        let hash = self.primary.store_block(block).await?;\n        \n        // Replicate based on consistency level\n        match self.config.consistency_level {\n            ConsistencyLevel::One => {\n                // Primary write is sufficient\n                return Ok(hash);\n            },\n            ConsistencyLevel::Quorum => {\n                let required_writes = (self.replicas.len() / 2) + 1;\n                let mut successful_writes = 1; // Primary already succeeded\n                \n                for replica in &mut self.replicas[..required_writes.min(self.replicas.len())] {\n                    if replica.store_block(block).await.is_ok() {\n                        successful_writes += 1;\n                    }\n                }\n                \n                if successful_writes >= required_writes {\n                    Ok(hash)\n                } else {\n                    Err(StorageError::InsufficientReplicas {\n                        required: required_writes,\n                        successful: successful_writes,\n                    })\n                }\n            },\n            ConsistencyLevel::All => {\n                // Write to all replicas\n                for replica in &mut self.replicas {\n                    replica.store_block(block).await?;\n                }\n                Ok(hash)\n            }\n        }\n    }\n    \n    pub async fn health_check(&self) -> HealthReport {\n        let mut report = HealthReport {\n            primary_healthy: false,\n            healthy_replicas: 0,\n            total_replicas: self.replicas.len(),\n            issues: Vec::new(),\n        };\n        \n        // Check primary\n        match self.primary.get_stats().await {\n            Ok(_) => report.primary_healthy = true,\n            Err(e) => report.issues.push(format!("Primary backend error: {}", e)),\n        }\n        \n        // Check replicas\n        for (i, replica) in self.replicas.iter().enumerate() {\n            match replica.get_stats().await {\n                Ok(_) => report.healthy_replicas += 1,\n                Err(e) => report.issues.push(format!("Replica {} error: {}", i, e)),\n            }\n        }\n        \n        report\n    }\n}\n\n#[derive(Debug)]\npub struct HealthReport {\n    pub primary_healthy: bool,\n    pub healthy_replicas: usize,\n    pub total_replicas: usize,\n    pub issues: Vec<String>,\n}\n\n// Usage example\nasync fn setup_enterprise_storage() -> Result<StorageManager> {\n    // Primary: High-performance RocksDB\n    let primary = Box::new(RocksDBBackend::new(RocksDBConfig {\n        path: "/data/primary/olocus".to_string(),\n        cache_size: 256 * 1024 * 1024, // 256MB\n        compression: CompressionType::Zstd,\n        ..Default::default()\n    })?);\n    \n    // Replica 1: SQLite for analytics\n    let replica1 = Box::new(SQLiteBackend::new(SQLiteConfig {\n        database_path: "/data/replica1/olocus.db".to_string(),\n        enable_wal: true,\n        journal_mode: JournalMode::WAL,\n        ..Default::default()\n    }).await?);\n    \n    // Replica 2: Filesystem for backup\n    let replica2 = Box::new(FilesystemBackend::new(\n        "/data/replica2/olocus",\n        CompressionType::Zstd\n    ).await?);\n    \n    let manager = StorageManager::new(\n        primary,\n        vec![replica1, replica2],\n        StorageManagerConfig {\n            replication_factor: 2,\n            consistency_level: ConsistencyLevel::Quorum,\n            health_check_interval: Duration::from_secs(30),\n            auto_failover: true,\n            sync_writes: false,\n        }\n    );\n    \n    Ok(manager)\n}\n'})}),"\n",(0,s.jsx)(n.h2,{id:"performance-benchmarks",children:"Performance Benchmarks"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-rust",children:'#[cfg(test)]\nmod storage_benchmarks {\n    use super::*;\n    use criterion::{black_box, Criterion, BenchmarkId};\n    \n    async fn benchmark_storage_backends(c: &mut Criterion) {\n        let backends = vec![\n            ("Memory", create_memory_backend()),\n            ("RocksDB", create_rocksdb_backend()),\n            ("SQLite", create_sqlite_backend().await),\n            ("Filesystem", create_filesystem_backend().await),\n        ];\n        \n        let test_blocks = create_test_blocks(1000);\n        \n        for (name, mut backend) in backends {\n            let group_name = format!("storage_{}", name);\n            let mut group = c.benchmark_group(group_name);\n            \n            // Single block operations\n            group.bench_function(BenchmarkId::new("store_single", name), |b| {\n                b.iter(|| async {\n                    backend.store_block(black_box(&test_blocks[0])).await\n                });\n            });\n            \n            group.bench_function(BenchmarkId::new("retrieve_single", name), |b| {\n                let hash = test_blocks[0].hash();\n                b.iter(|| async {\n                    backend.retrieve_block(black_box(&hash)).await\n                });\n            });\n            \n            // Batch operations\n            group.bench_function(BenchmarkId::new("store_batch", name), |b| {\n                b.iter(|| async {\n                    backend.store_blocks(black_box(&test_blocks[0..100])).await\n                });\n            });\n            \n            // Query operations\n            group.bench_function(BenchmarkId::new("list_blocks", name), |b| {\n                let options = ListOptions {\n                    limit: Some(100),\n                    order: SortOrder::ByIndex,\n                    ..Default::default()\n                };\n                b.iter(|| async {\n                    backend.list_blocks(black_box(&options)).await\n                });\n            });\n            \n            group.finish();\n        }\n    }\n    \n    fn create_test_blocks(count: usize) -> Vec<Block<TestPayload>> {\n        (0..count).map(|i| {\n            let payload = TestPayload {\n                id: i as u64,\n                data: format!("test_data_{}", i),\n                timestamp: SystemTime::now(),\n            };\n            \n            create_test_block(payload, i as u64)\n        }).collect()\n    }\n}\n\n// Performance results (approximate)\nstruct StoragePerformanceMetrics {\n    memory: BackendMetrics {\n        single_store: Duration::from_micros(10),     // 10\u03bcs\n        single_retrieve: Duration::from_micros(5),   // 5\u03bcs\n        batch_store_100: Duration::from_millis(1),   // 1ms\n        list_100: Duration::from_micros(100),        // 100\u03bcs\n    },\n    rocksdb: BackendMetrics {\n        single_store: Duration::from_micros(50),     // 50\u03bcs\n        single_retrieve: Duration::from_micros(20),  // 20\u03bcs\n        batch_store_100: Duration::from_millis(3),   // 3ms\n        list_100: Duration::from_millis(1),          // 1ms\n    },\n    sqlite: BackendMetrics {\n        single_store: Duration::from_micros(100),    // 100\u03bcs\n        single_retrieve: Duration::from_micros(80),  // 80\u03bcs\n        batch_store_100: Duration::from_millis(8),   // 8ms\n        list_100: Duration::from_millis(2),          // 2ms\n    },\n    filesystem: BackendMetrics {\n        single_store: Duration::from_millis(1),      // 1ms\n        single_retrieve: Duration::from_micros(500), // 500\u03bcs\n        batch_store_100: Duration::from_millis(100), // 100ms\n        list_100: Duration::from_millis(10),         // 10ms\n    },\n}\n'})}),"\n",(0,s.jsx)(n.h2,{id:"related-documentation",children:"Related Documentation"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"/docs/extensions/infrastructure/query-engine",children:"Query Engine"})," - Advanced querying capabilities"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"/docs/extensions/infrastructure/http-api",children:"HTTP API"})," - REST API for storage operations"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"/docs/extensions/infrastructure/metrics",children:"Metrics"})," - Storage performance monitoring"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"/docs/extensions/infrastructure/network",children:"Network"})," - Distributed storage coordination"]}),"\n"]})]})}function _(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>i,x:()=>l});var a=t(6540);const s={},o=a.createContext(s);function i(e){const n=a.useContext(o);return a.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:i(e.components),a.createElement(o.Provider,{value:n},e.children)}}}]);